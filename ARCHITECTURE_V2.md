# Informe de Arquitectura - PROCUREDATA v2
## Espacio de Datos Industrial con FIWARE + Kubernetes

**Versi√≥n**: 2.0  
**Fecha**: Diciembre 2024  
**Autor**: Equipo de Arquitectura PROCUREDATA  
**Estado**: Producci√≥n-Ready

---

## üìã Executive Summary

PROCUREDATA v2 implementa una arquitectura **h√≠brida de 3 capas** para gestionar el ciclo de vida completo de transacciones de datos industriales bajo el marco de **soberan√≠a de datos IDS** (International Data Spaces):

1. **Capa de Presentaci√≥n**: Frontend React/Vite desplegado en Lovable
2. **Capa de L√≥gica de Negocio**: Supabase (PostgreSQL + Edge Functions)
3. **Capa de Contexto en Tiempo Real**: FIWARE (Orion-LD + Keyrock + TRUE Connector) en Kubernetes

Esta separaci√≥n de responsabilidades garantiza:
- ‚úÖ **Escalabilidad**: Cada capa escala independientemente
- ‚úÖ **Seguridad**: Credenciales FIWARE nunca expuestas al navegador
- ‚úÖ **Interoperabilidad**: Compatibilidad con est√°ndares NGSI-LD y ODRL 2.0

---

## üèóÔ∏è 1. Arquitectura de Base de Datos H√≠brida

### 1.1 Principio de Separaci√≥n de Responsabilidades

La arquitectura utiliza **3 bases de datos especializadas**, cada una optimizada para su caso de uso:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       CAPA DE APLICACI√ìN                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ        PostgreSQL (Supabase)                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Usuarios, Perfiles, Organizaciones                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Transacciones hist√≥ricas (data_transactions)         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Cat√°logo de activos (data_assets)                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Estado UI (preferencias, roles)                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Persistencia: SSD, Backups diarios                     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   CAPA DE CONTEXTO EN TIEMPO REAL               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ        MongoDB (Orion-LD)                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Gemelos Digitales (Digital Twins)                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Entidades NGSI-LD (Devices, Sensors, Buildings)     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Estado actual de activos f√≠sicos                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Suscripciones y notificaciones                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Persistencia: R√©plicas 3x, WiredTiger Cache 1.5GB    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     CAPA DE IDENTIDAD Y ACCESO                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ        MySQL (Keyrock)                                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Credenciales OAuth2 (Client ID/Secret)              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Tokens de acceso (X-Subject-Token)                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Pol√≠ticas XACML (permisos granulares)               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Aplicaciones registradas                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Persistencia: Master-Replica, Logs binarios          ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.2 Justificaci√≥n de la Separaci√≥n

#### PostgreSQL (Supabase) - "El Registro Oficial"
- **Prop√≥sito**: Datos estructurados de negocio con integridad ACID
- **Casos de uso**:
  - Auditor√≠a de transacciones (qui√©n, cu√°ndo, qu√©)
  - Relaciones entre organizaciones (RBAC)
  - Metadata de activos (descripci√≥n, tipo, propietario)
- **Ventajas**:
  - Foreign Keys para garantizar consistencia
  - RLS (Row Level Security) integrado
  - Backups point-in-time autom√°ticos
- **Acceso**: Directo desde frontend v√≠a Supabase SDK

**Ejemplo pr√°ctico**: Cuando un consumidor solicita acceso a un dataset:
```sql
-- PostgreSQL: Registra la transacci√≥n de negocio
INSERT INTO data_transactions (
  asset_id, 
  consumer_org_id, 
  requested_by, 
  status
) VALUES (
  'asset-001', 
  'org-consumer-123', 
  'user@company.com', 
  'pending_subject'
);
```

**¬øPor qu√© PostgreSQL?**
- ‚úÖ Garantiza que **no se pierda ninguna transacci√≥n** (WAL + ACID)
- ‚úÖ Foreign Keys validan que `asset_id` existe antes de insertar
- ‚úÖ RLS policies impiden que org-consumer-123 vea transacciones de otras organizaciones
- ‚úÖ Backups autom√°ticos permiten auditor√≠as hist√≥ricas (GDPR compliance)

---

#### MongoDB (Orion-LD) - "El Cerebro en Tiempo Real"
- **Prop√≥sito**: Estado din√°mico y mutable de activos f√≠sicos
- **Casos de uso**:
  - Temperatura actual de un sensor: `23.5¬∞C ‚Üí 24.1¬∞C` (cada 5s)
  - Ubicaci√≥n GPS de un veh√≠culo: actualizaci√≥n continua
  - Agregaciones geoespaciales: sensores en radio de 5km
- **Ventajas**:
  - Esquema flexible (JSON-like documents)
  - Queries geoespaciales nativas
  - Alta escritura concurrente (10,000+ writes/s)
- **Acceso**: **Solo** v√≠a Edge Function `fiware-proxy` (nunca directo)

**Ejemplo pr√°ctico**: El sensor reporta nueva temperatura cada 5 segundos:
```json
// MongoDB: Actualiza el estado actual del sensor (gemelo digital)
PATCH /ngsi-ld/v1/entities/urn:ngsi-ld:Sensor:001/attrs
{
  "temperature": {
    "type": "Property",
    "value": 24.1,
    "unitCode": "CEL",
    "observedAt": "2024-12-01T10:05:23Z"
  }
}
```

**¬øPor qu√© MongoDB?**
- ‚úÖ **Alta frecuencia de escritura**: Optimizado para IoT (miles de updates/segundo)
- ‚úÖ **Esquema flexible**: Los sensores pueden agregar atributos sin migraciones (ej: agregar `humidity`)
- ‚úÖ **Queries geoespaciales**: "Encuentra todos los sensores en 5km de Madrid" es nativo
- ‚úÖ **Time-to-Live (TTL)**: Datos antiguos se eliminan autom√°ticamente (ej: temperatura hace 30 d√≠as)

**Anti-Patr√≥n**: ‚ùå Nunca almacenar transacciones hist√≥ricas en MongoDB. Son datos de auditor√≠a y pertenecen a PostgreSQL.

---

#### MySQL (Keyrock) - "El Guardi√°n"
- **Prop√≥sito**: Gesti√≥n de identidades y tokens OAuth2
- **Casos de uso**:
  - Generar tokens de sesi√≥n para usuarios
  - Validar permisos de aplicaciones externas
  - Federaci√≥n de identidades (SSO con otros espacios de datos)
- **Ventajas**:
  - Est√°ndar en FIWARE (amplia compatibilidad)
  - Integraci√≥n nativa con PEP-Proxy
- **Acceso**: Interno, gestionado por Keyrock API

**Ejemplo pr√°ctico**: Usuario bot solicita token para acceder a Orion:
```sql
-- MySQL: Keyrock valida credenciales y genera token
SELECT id, password_hash FROM user WHERE email = 'bot@procuredata.com';
INSERT INTO oauth_token (user_id, token, expires_at) 
VALUES ('bot-001', 'xyz789', NOW() + INTERVAL 1 HOUR);
```

**¬øPor qu√© MySQL?**
- ‚úÖ **Est√°ndar FIWARE**: Keyrock est√° dise√±ado para MySQL (migraci√≥n compleja a otros DBs)
- ‚úÖ **Transacciones ACID**: Tokens nunca se duplican (unique constraints)
- ‚úÖ **Replicaci√≥n Master-Slave**: Alta disponibilidad sin p√©rdida de sesiones
- ‚úÖ **XACML Policies**: Permisos granulares (ej: "usuario X puede leer sensores de tipo Y")

**Separaci√≥n cr√≠tica**: ‚ùå Nunca almacenar tokens OAuth en PostgreSQL. Keyrock los gestiona con rotaci√≥n autom√°tica y expiraci√≥n.

---

### 1.2.1 ¬øPor qu√© NO usar una sola base de datos?

**Problema 1: Diferentes patrones de acceso**
```
PostgreSQL (transacciones):
‚îú‚îÄ Writes: Bajos (100/min) ‚Üí Optimizado para consistencia
‚îî‚îÄ Reads: Medianos (500/min) ‚Üí Queries complejos con JOINs

MongoDB (gemelos digitales):
‚îú‚îÄ Writes: Altos (10,000/min) ‚Üí Optimizado para throughput
‚îî‚îÄ Reads: Muy altos (50,000/min) ‚Üí Queries simples sin JOINs
```

Si mezclamos en PostgreSQL:
- ‚ùå Las 10,000 escrituras/min de sensores saturar√≠an el WAL
- ‚ùå Los queries complejos de auditor√≠a competir√≠an con lecturas de IoT
- ‚ùå Lock contention: escrituras de sensores bloquear√≠an lecturas de dashboard

**Problema 2: Diferentes requisitos de backup**
```
PostgreSQL: Backup completo diario + WAL continuo (7 d√≠as retenci√≥n)
MongoDB: Snapshot cada 6 horas + oplog (24h retenci√≥n) + TTL autom√°tico
MySQL: Backup binlog cada hora (tokens no necesitan long-term storage)
```

**Problema 3: Diferentes modelos de escalado**
```
PostgreSQL: Escalado vertical (CPU/RAM m√°s potente)
MongoDB: Escalado horizontal (sharding por ubicaci√≥n geogr√°fica)
MySQL: Replicaci√≥n read-only para validaci√≥n de tokens
```

### 1.3 Flujo de Datos Completo

```mermaid
sequenceDiagram
    participant U as Usuario
    participant F as Frontend (Lovable)
    participant SB as Supabase (PostgreSQL)
    participant EF as Edge Function
    participant K as Keyrock (MySQL)
    participant O as Orion-LD (MongoDB)

    Note over U,F: 1. Usuario inicia transacci√≥n
    U->>F: Solicita acceso a "IoT Dataset"
    F->>SB: INSERT INTO data_transactions (...)
    SB-->>F: Transaction ID: abc-123
    
    Note over F,EF: 2. Frontend consulta estado en tiempo real
    F->>EF: invoke('fiware-proxy', { path: '/entities' })
    EF->>K: POST /v1/auth/tokens
    K-->>EF: X-Subject-Token: xyz-789
    EF->>O: GET /entities (+ Token)
    O-->>EF: [Device entities con temperatura actual]
    EF-->>F: Datos normalizados
    
    Note over F,SB: 3. Frontend actualiza UI y Postgres
    F->>F: Renderiza dashboard con datos vivos
    F->>SB: UPDATE data_transactions SET status='approved'
```

### 1.4 Pol√≠tica de Sincronizaci√≥n

**Regla de Oro**: PostgreSQL es la **fuente de verdad** para transacciones hist√≥ricas. MongoDB es la **fuente de verdad** para estado actual.

| Evento | Acci√≥n en PostgreSQL | Acci√≥n en MongoDB |
|--------|---------------------|-------------------|
| Nueva transacci√≥n creada | `INSERT INTO data_transactions` | *(Ninguna)* |
| Transacci√≥n aprobada | `UPDATE status='approved'` | *(Ninguna)* |
| Data asset compartido | `INSERT INTO data_assets` | `POST /ngsi-ld/v1/entities` (crear DataAsset entity) |
| Sensor reporta nuevo valor | *(Ninguna)* | `PATCH /entities/{id}/attrs` (actualizar temperatura) |
| Transacci√≥n completada | `UPDATE status='completed'` | `POST /subscriptions` (notificar a consumidor) |

**Anti-Patr√≥n**: ‚ùå Nunca duplicar datos est√°ticos (ej: nombre de organizaci√≥n) en MongoDB. Usa `Relationship` para referenciar.

---

## üöÄ 2. Estrategia de Despliegue en Kubernetes

### 2.1 Ventajas de Migrar a Kubernetes

El `docker-compose.yml` actual es ideal para desarrollo, pero producci√≥n requiere:

| Problema en Docker Compose | Soluci√≥n en Kubernetes |
|-----------------------------|------------------------|
| Reinicio manual tras fallos | `restartPolicy: Always` autom√°tico |
| Escalado manual (1 r√©plica) | `HorizontalPodAutoscaler` (2-10 r√©plicas) |
| Sin balanceo de carga interno | `Service` tipo `ClusterIP` con load balancer |
| Actualizaciones con downtime | `RollingUpdate` sin interrupciones |
| Secretos en texto plano | `Secrets` encriptados con etcd |

### 2.2 Arquitectura de Pods Propuesta

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      NAMESPACE: fiware-prod                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ  StatefulSet    ‚îÇ    ‚îÇ  StatefulSet    ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  mongo-orion    ‚îÇ    ‚îÇ  mysql-keyrock  ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  (3 replicas)   ‚îÇ    ‚îÇ  (2 replicas)   ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  PVC: 50Gi SSD  ‚îÇ    ‚îÇ  PVC: 20Gi SSD  ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ  Deployment: orion-ld (4 replicas)      ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  Resources: CPU 500m, RAM 1Gi           ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  Env: DBHOST=mongo-orion-service        ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ  Deployment:     ‚îÇ    ‚îÇ  Deployment:     ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  keyrock         ‚îÇ    ‚îÇ  pep-proxy       ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  (2 replicas)    ‚îÇ    ‚îÇ  (3 replicas)    ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  Port: 3005      ‚îÇ    ‚îÇ  Port: 1027      ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ  ‚îÇ  Deployment: true-connector              ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ  Pods: ecc + data-app (sidecar pattern) ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ  Resources: CPU 1000m, RAM 2Gi           ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ  PVC: 10Gi para certificados IDS         ‚îÇ               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ  ‚îÇ  Ingress: nginx-ingress-controller       ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ  TLS: cert-manager (Let's Encrypt)       ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ  Routes:                                 ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ    /orion ‚Üí     Service: pep-proxy:1027  ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ    /keyrock ‚Üí   Service: keyrock:3005    ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ    /connector ‚Üí Service: true-ecc:8080   ‚îÇ               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.3 Manifests de Kubernetes (Ejemplo: Orion-LD)

#### StatefulSet para MongoDB

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongo-orion
  namespace: fiware-prod
spec:
  serviceName: mongo-orion-service
  replicas: 3
  selector:
    matchLabels:
      app: mongo-orion
  template:
    metadata:
      labels:
        app: mongo-orion
    spec:
      containers:
      - name: mongo
        image: mongo:4.4
        command:
          - mongod
          - --wiredTigerCacheSizeGB
          - "1.5"
          - --nojournal
          - --replSet
          - rs0
        ports:
        - containerPort: 27017
        volumeMounts:
        - name: mongo-data
          mountPath: /data/db
        resources:
          requests:
            cpu: 500m
            memory: 2Gi
          limits:
            cpu: 2000m
            memory: 4Gi
  volumeClaimTemplates:
  - metadata:
      name: mongo-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 50Gi
```

#### Deployment para Orion-LD

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: orion-ld
  namespace: fiware-prod
spec:
  replicas: 4
  selector:
    matchLabels:
      app: orion-ld
  template:
    metadata:
      labels:
        app: orion-ld
    spec:
      containers:
      - name: orion
        image: fiware/orion-ld:latest
        args:
          - -dbhost
          - mongo-orion-service
          - -logLevel
          - WARN
          - -forwarding
          - "true"
        ports:
        - containerPort: 1026
        livenessProbe:
          httpGet:
            path: /version
            port: 1026
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /version
            port: 1026
          initialDelaySeconds: 5
          periodSeconds: 5
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi
```

### 2.4 Ingress Controller con TLS

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: fiware-ingress
  namespace: fiware-prod
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
spec:
  tls:
  - hosts:
    - api.procuredata.com
    secretName: fiware-tls-secret
  rules:
  - host: api.procuredata.com
    http:
      paths:
      - path: /orion
        pathType: Prefix
        backend:
          service:
            name: pep-proxy-service
            port:
              number: 1027
      - path: /keyrock
        pathType: Prefix
        backend:
          service:
            name: keyrock-service
            port:
              number: 3005
      - path: /connector
        pathType: Prefix
        backend:
          service:
            name: true-ecc-service
            port:
              number: 8080
```

### 2.5 HorizontalPodAutoscaler (HPA)

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: orion-ld-hpa
  namespace: fiware-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: orion-ld
  minReplicas: 4
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

**Comportamiento**: Si la CPU promedio supera 70%, K8s escala de 4 a 10 r√©plicas autom√°ticamente.

### 2.6 Gu√≠a Pr√°ctica de Migraci√≥n: Docker Compose ‚Üí Kubernetes

#### Paso 1: An√°lisis del docker-compose.yml actual

Antes de migrar, identifica qu√© componentes necesitan persistencia y cu√°les son stateless:

```bash
# Componentes que requieren StatefulSets (datos persistentes):
‚îú‚îÄ mongo-db ‚Üí StatefulSet (3 r√©plicas con MongoDB Replica Set)
‚îú‚îÄ mysql-db ‚Üí StatefulSet (2 r√©plicas Master-Slave)

# Componentes que usan Deployments (stateless, pueden reiniciar):
‚îú‚îÄ orion ‚Üí Deployment (4 r√©plicas con HPA)
‚îú‚îÄ keyrock ‚Üí Deployment (2 r√©plicas)
‚îú‚îÄ pep-proxy ‚Üí Deployment (3 r√©plicas)
‚îú‚îÄ true-connector-ecc ‚Üí Deployment (2 r√©plicas)
‚îî‚îÄ true-connector-data-app ‚Üí Sidecar con ECC

# Networking:
‚îú‚îÄ Services ClusterIP para discovery interno
‚îú‚îÄ Ingress para acceso externo HTTPS
‚îî‚îÄ NetworkPolicies para aislamiento de red
```

#### Paso 2: Estructura de directorios recomendada

```
k8s/
‚îú‚îÄ‚îÄ 00-namespace.yaml
‚îú‚îÄ‚îÄ 01-storage/
‚îÇ   ‚îú‚îÄ‚îÄ storageclass-fast-ssd.yaml
‚îÇ   ‚îú‚îÄ‚îÄ mongo-pvc.yaml
‚îÇ   ‚îî‚îÄ‚îÄ mysql-pvc.yaml
‚îú‚îÄ‚îÄ 02-secrets/
‚îÇ   ‚îú‚îÄ‚îÄ keyrock-db-secret.yaml
‚îÇ   ‚îú‚îÄ‚îÄ pep-proxy-secret.yaml
‚îÇ   ‚îî‚îÄ‚îÄ true-connector-certs-secret.yaml
‚îú‚îÄ‚îÄ 03-databases/
‚îÇ   ‚îú‚îÄ‚îÄ mongo-statefulset.yaml
‚îÇ   ‚îú‚îÄ‚îÄ mongo-service.yaml
‚îÇ   ‚îú‚îÄ‚îÄ mysql-statefulset.yaml
‚îÇ   ‚îî‚îÄ‚îÄ mysql-service.yaml
‚îú‚îÄ‚îÄ 04-fiware/
‚îÇ   ‚îú‚îÄ‚îÄ orion-deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ orion-service.yaml
‚îÇ   ‚îú‚îÄ‚îÄ orion-hpa.yaml
‚îÇ   ‚îú‚îÄ‚îÄ keyrock-deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ keyrock-service.yaml
‚îÇ   ‚îú‚îÄ‚îÄ pep-proxy-deployment.yaml
‚îÇ   ‚îî‚îÄ‚îÄ pep-proxy-service.yaml
‚îú‚îÄ‚îÄ 05-ids/
‚îÇ   ‚îú‚îÄ‚îÄ true-connector-deployment.yaml
‚îÇ   ‚îî‚îÄ‚îÄ true-connector-service.yaml
‚îú‚îÄ‚îÄ 06-networking/
‚îÇ   ‚îú‚îÄ‚îÄ ingress.yaml
‚îÇ   ‚îî‚îÄ‚îÄ network-policies.yaml
‚îî‚îÄ‚îÄ README.md
```

#### Paso 3: Migrar secrets de docker-compose a Kubernetes Secrets

**‚ùå En docker-compose.yml (texto plano inseguro):**
```yaml
environment:
  - PEP_PROXY_APP_ID=7a8b9c0d-1234-5678
  - PEP_PROXY_PASSWORD=insecure_password  
  - MYSQL_ROOT_PASSWORD=root123
```

**‚úÖ En Kubernetes (encriptado en etcd):**
```bash
# Generar secrets seguros
kubectl create secret generic pep-proxy-credentials \
  --from-literal=app-id=$(uuidgen) \
  --from-literal=password=$(openssl rand -base64 32) \
  --namespace=fiware-prod

kubectl create secret generic keyrock-db-credentials \
  --from-literal=root-password=$(openssl rand -base64 32) \
  --from-literal=user-password=$(openssl rand -base64 32) \
  --namespace=fiware-prod
```

**Uso en manifests:**
```yaml
# k8s/04-fiware/pep-proxy-deployment.yaml
env:
- name: PEP_PROXY_APP_ID
  valueFrom:
    secretKeyRef:
      name: pep-proxy-credentials
      key: app-id
- name: PEP_PROXY_PASSWORD
  valueFrom:
    secretKeyRef:
      name: pep-proxy-credentials
      key: password
```

#### Paso 4: Migrar datos de Docker volumes a PersistentVolumes

**Backup de datos Docker:**
```bash
# 1. Backup MongoDB
docker exec db-mongo mongodump --out /tmp/backup
docker cp db-mongo:/tmp/backup ./mongo-backup-$(date +%Y%m%d)

# 2. Backup MySQL
docker exec db-mysql mysqldump -u root -p idm > mysql-backup-$(date +%Y%m%d).sql

# 3. Comprimir backups
tar -czf fiware-backup-$(date +%Y%m%d).tar.gz mongo-backup-* mysql-backup-*
```

**Restaurar en Kubernetes:**
```bash
# 1. Copiar backup al pod de MongoDB
kubectl cp mongo-backup-20241201 mongo-orion-0:/tmp/backup -n fiware-prod

# 2. Ejecutar restore
kubectl exec -it mongo-orion-0 -n fiware-prod -- \
  mongorestore --host mongo-orion-service /tmp/backup

# 3. Verificar datos
kubectl exec -it mongo-orion-0 -n fiware-prod -- \
  mongosh --eval "db.entities.countDocuments()"
```

#### Paso 5: Configurar networking (Docker bridge ‚Üí K8s Services)

**Docker Compose:**
```yaml
networks:
  data_space_net:
    driver: bridge  # DNS interno: ping mongo-db
```

**Kubernetes equivalente:**
```yaml
# Namespace proporciona aislamiento (reemplaza Docker network)
apiVersion: v1
kind: Namespace
metadata:
  name: fiware-prod
---
# Service proporciona DNS interno
apiVersion: v1
kind: Service
metadata:
  name: mongo-orion-service
  namespace: fiware-prod
spec:
  selector:
    app: mongo-orion
  ports:
  - port: 27017
    targetPort: 27017
  clusterIP: None  # Headless para StatefulSet
---
# Ahora Orion puede conectar con:
# mongodb://mongo-orion-service.fiware-prod.svc.cluster.local:27017/orion
```

#### Paso 6: Exponer servicios (Ports ‚Üí Ingress)

**Docker Compose (puertos expuestos al host):**
```yaml
ports:
  - "3005:3005"  # Keyrock UI
  - "1027:1027"  # PEP-Proxy
  - "8080:8080"  # TRUE Connector
```

**Kubernetes (Ingress con TLS autom√°tico):**
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: fiware-ingress
  namespace: fiware-prod
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "1000"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
  - hosts:
    - api.procuredata.com
    secretName: fiware-tls-cert  # Auto-generado por cert-manager
  rules:
  - host: api.procuredata.com
    http:
      paths:
      - path: /orion
        pathType: Prefix
        backend:
          service:
            name: pep-proxy-service
            port:
              number: 1027
      - path: /keyrock
        pathType: Prefix
        backend:
          service:
            name: keyrock-service
            port:
              number: 3005
      - path: /connector
        pathType: Prefix
        backend:
          service:
            name: true-ecc-service
            port:
              number: 8080
```

**Ventajas del Ingress:**
- ‚úÖ SSL/TLS autom√°tico con Let's Encrypt
- ‚úÖ Rate limiting (1000 req/min por IP)
- ‚úÖ Path-based routing (m√∫ltiples servicios en un dominio)
- ‚úÖ Load balancing autom√°tico entre r√©plicas de pods

#### Paso 7: Despliegue secuencial

```bash
# 1. Crear namespace
kubectl apply -f k8s/00-namespace.yaml

# 2. Configurar storage
kubectl apply -f k8s/01-storage/

# 3. Crear secrets
kubectl apply -f k8s/02-secrets/

# 4. Desplegar bases de datos (esperar a que est√©n Ready)
kubectl apply -f k8s/03-databases/
kubectl wait --for=condition=ready pod -l app=mongo-orion -n fiware-prod --timeout=300s
kubectl wait --for=condition=ready pod -l app=mysql-keyrock -n fiware-prod --timeout=300s

# 5. Restaurar datos (si es migraci√≥n)
kubectl cp mongo-backup-20241201 mongo-orion-0:/tmp/backup -n fiware-prod
kubectl exec -it mongo-orion-0 -n fiware-prod -- mongorestore /tmp/backup

# 6. Desplegar componentes FIWARE
kubectl apply -f k8s/04-fiware/
kubectl wait --for=condition=available deployment -l tier=fiware -n fiware-prod --timeout=300s

# 7. Desplegar TRUE Connector
kubectl apply -f k8s/05-ids/

# 8. Configurar networking
kubectl apply -f k8s/06-networking/

# 9. Verificar todo
kubectl get all -n fiware-prod
```

#### Paso 8: Validaci√≥n post-migraci√≥n

```bash
# 1. Verificar pods
kubectl get pods -n fiware-prod
# Todos deben estar en estado "Running" y "Ready 1/1"

# 2. Test de conectividad interna
kubectl run -it test-pod --image=busybox --rm -n fiware-prod -- sh
# Dentro del pod:
nslookup mongo-orion-service
wget -O- http://orion-ld-service:1026/version
wget -O- http://keyrock-service:3005/version

# 3. Test de conectividad externa (Ingress)
curl https://api.procuredata.com/orion/version
curl https://api.procuredata.com/keyrock/version

# 4. Verificar datos migrados
kubectl exec -it mongo-orion-0 -n fiware-prod -- \
  mongosh --eval "db.entities.find().limit(5).pretty()"

# 5. Monitorear logs
kubectl logs -f deployment/orion-ld -n fiware-prod
kubectl logs -f deployment/pep-proxy -n fiware-prod
```

#### Paso 9: Rollback plan (si algo falla)

```bash
# Opci√≥n 1: Rollback a versi√≥n anterior de deployment
kubectl rollout undo deployment/orion-ld -n fiware-prod

# Opci√≥n 2: Eliminar todo y volver a Docker Compose
kubectl delete namespace fiware-prod

# Restaurar Docker Compose
cd ~/procuredata-fiware
docker compose down
docker volume rm procuredata_mongo-db procuredata_mysql-db
docker compose up -d
# Restaurar backups
docker exec -i db-mongo mongorestore /tmp/backup
docker exec -i db-mysql mysql -u root -p idm < mysql-backup-20241201.sql
```

#### Comparativa: Antes vs Despu√©s

| Aspecto | Docker Compose | Kubernetes |
|---------|---------------|-----------|
| **Despliegue inicial** | 5 minutos | 30 minutos (setup √∫nico) |
| **Alta disponibilidad** | ‚ùå Single host | ‚úÖ Multi-nodo |
| **Auto-scaling** | ‚ùå Manual | ‚úÖ HPA autom√°tico (2-10 r√©plicas) |
| **SSL/TLS** | ‚ùå Manual (nginx externo) | ‚úÖ Autom√°tico (cert-manager) |
| **Secrets management** | ‚ùå Texto plano en .env | ‚úÖ Encriptados en etcd |
| **Health checks** | ‚ùå Restart manual | ‚úÖ Liveness/Readiness probes |
| **Rollback** | ‚ùå Redeploy completo | ‚úÖ `kubectl rollout undo` |
| **Backup** | ‚ùå Script manual | ‚úÖ Velero autom√°tico |
| **Monitoreo** | ‚ùå `docker stats` | ‚úÖ Prometheus + Grafana |
| **Networking** | ‚ùå Bridge b√°sico | ‚úÖ NetworkPolicies (Zero Trust) |
| **Costo mensual** | $50 (1 VPS) | $270-462 (3-node cluster) |

**Recomendaci√≥n**: 
- **Desarrollo/Testing**: Docker Compose
- **Producci√≥n**: Kubernetes

---

---

## üîÑ 3. Flujo de Datos Completo (Producci√≥n)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Usuario Final  ‚îÇ
‚îÇ (Navegador)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ HTTPS
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Lovable (Frontend)     ‚îÇ
‚îÇ React + Vite           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ HTTPS (Supabase Auth)
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Supabase Edge Function (Proxy)    ‚îÇ
‚îÇ ‚Ä¢ Cache de tokens (1h TTL)         ‚îÇ
‚îÇ ‚Ä¢ Inyecta X-Auth-Token             ‚îÇ
‚îÇ ‚Ä¢ Normaliza NGSI-LD ‚Üí JSON plano   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ HTTPS
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Ingress Controller (K8s)           ‚îÇ
‚îÇ ‚Ä¢ SSL/TLS Termination              ‚îÇ
‚îÇ ‚Ä¢ Rate Limiting (1000 req/min)     ‚îÇ
‚îÇ ‚Ä¢ Load Balancing (Round-robin)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ HTTP (cluster interno)
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Service: pep-proxy (ClusterIP)    ‚îÇ
‚îÇ ‚Ä¢ Endpoints: [pod1, pod2, pod3]    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PEP-Proxy Pods (3 r√©plicas)        ‚îÇ
‚îÇ ‚Ä¢ Valida X-Auth-Token con Keyrock  ‚îÇ
‚îÇ ‚Ä¢ Enforce pol√≠ticas XACML          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Service: orion-ld (ClusterIP)      ‚îÇ
‚îÇ ‚Ä¢ Endpoints: [pod1, pod2, pod3...] ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Orion-LD Pods (4-10 r√©plicas)      ‚îÇ
‚îÇ ‚Ä¢ Ejecuta query NGSI-LD            ‚îÇ
‚îÇ ‚Ä¢ Lee/escribe en MongoDB           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ StatefulSet: MongoDB (3 r√©plicas)  ‚îÇ
‚îÇ ‚Ä¢ Replica Set con leader election  ‚îÇ
‚îÇ ‚Ä¢ Persistencia: PVC 50Gi SSD       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.1 Ejemplo de Latencia End-to-End

| Paso | Componente | Latencia |
|------|-----------|----------|
| 1 | Browser ‚Üí Lovable | 50ms (CDN) |
| 2 | Lovable ‚Üí Supabase (Edge Fn) | 20ms |
| 3 | Edge Fn ‚Üí Keyrock (token cache) | 0ms (cached) |
| 4 | Edge Fn ‚Üí Ingress K8s | 30ms |
| 5 | Ingress ‚Üí PEP-Proxy | 5ms |
| 6 | PEP-Proxy ‚Üí Orion-LD | 10ms |
| 7 | Orion-LD ‚Üí MongoDB | 15ms |
| **Total** | | **130ms** |

**Optimizaciones aplicadas**:
- ‚úÖ Token cacheado (ahorra 50ms vs. Keyrock cada vez)
- ‚úÖ R√©plicas de Orion-LD (reduce latencia bajo carga)
- ‚úÖ MongoDB indexes en `id` y `type` (queries r√°pidas)

---

## üîí 4. Seguridad en Profundidad (Defense in Depth)

### 4.1 Capa de Red (Network Policies)

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: orion-ld-policy
  namespace: fiware-prod
spec:
  podSelector:
    matchLabels:
      app: orion-ld
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: pep-proxy  # Solo PEP-Proxy puede conectar a Orion
    ports:
    - protocol: TCP
      port: 1026
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: mongo-orion  # Solo puede hablar con MongoDB
    ports:
    - protocol: TCP
      port: 27017
```

**Resultado**: Orion-LD est√° **aislado**. Ni siquiera otros pods del namespace pueden conectar.

### 4.2 Gesti√≥n de Secretos

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: keyrock-db-credentials
  namespace: fiware-prod
type: Opaque
stringData:
  MYSQL_ROOT_PASSWORD: "$(openssl rand -base64 32)"  # Generado din√°micamente
  MYSQL_PASSWORD: "$(openssl rand -base64 32)"
```

**Ventajas vs. docker-compose**:
- ‚úÖ Secretos encriptados en `etcd` (K8s control plane)
- ‚úÖ Rotaci√≥n autom√°tica con `cert-manager`
- ‚úÖ RBAC: Solo el pod `keyrock` puede leer este secret

### 4.3 Auditor√≠a de Accesos

**Herramientas recomendadas**:
- **Falco**: Detecta comportamientos an√≥malos (ej: pod ejecutando `curl` inesperado)
- **Open Policy Agent (OPA)**: Pol√≠ticas de admisi√≥n (ej: "No pods privilegiados")
- **Loki**: Centraliza logs de todos los pods

Ejemplo de alerta Falco:
```yaml
- rule: Unauthorized Access to MongoDB
  desc: Detectar conexiones a MongoDB desde pods no autorizados
  condition: >
    connection_made and
    container.image.repository != "fiware/orion-ld" and
    fd.sip = "mongo-orion-service"
  output: "Conexi√≥n sospechosa a MongoDB (pod=%container.name ip=%fd.cip)"
  priority: CRITICAL
```

---

## üìä 5. Monitoreo y Observabilidad

### 5.1 Stack Recomendado: Prometheus + Grafana

```yaml
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: orion-ld-monitor
  namespace: fiware-prod
spec:
  selector:
    matchLabels:
      app: orion-ld
  endpoints:
  - port: metrics
    interval: 30s
```

**M√©tricas clave a monitorear**:

| M√©trica | Alerta si... |
|---------|--------------|
| `orion_entities_total` | < 1000 (dataset vac√≠o) |
| `orion_query_latency_p99` | > 500ms (slow queries) |
| `pep_proxy_401_errors` | > 10/min (auth issues) |
| `mongo_replica_lag_seconds` | > 10s (replicaci√≥n lenta) |
| `keyrock_token_generation_rate` | > 1000/min (posible DDoS) |

### 5.2 Dashboards Grafana Predefinidos

**Dashboard 1: Context Broker Health**
```json
{
  "panels": [
    {
      "title": "Entidades NGSI-LD por Tipo",
      "targets": [
        {
          "expr": "sum by (type) (orion_entities_total)",
          "legendFormat": "{{ type }}"
        }
      ]
    },
    {
      "title": "Latencia de Queries (P50, P95, P99)",
      "targets": [
        {
          "expr": "histogram_quantile(0.99, orion_query_duration_seconds_bucket)"
        }
      ]
    }
  ]
}
```

**Dashboard 2: TRUE Connector (IDS)**
- Contratos activos (`true_connector_contracts_active`)
- Datos transferidos a otros espacios (`true_connector_bytes_transferred`)
- Validaciones DAPS exitosas/fallidas

---

## üß™ 6. Testing y CI/CD

### 6.1 Pipeline GitOps (ArgoCD)

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: fiware-stack
  namespace: argocd
spec:
  project: production
  source:
    repoURL: https://github.com/procuredata/fiware-k8s
    targetRevision: main
    path: manifests/production
  destination:
    server: https://kubernetes.default.svc
    namespace: fiware-prod
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
```

**Flujo**:
1. Dev hace `git push` a `fiware-k8s/manifests/production/orion-deployment.yaml`
2. ArgoCD detecta cambio (polling cada 3min)
3. Aplica cambios a K8s con estrategia `RollingUpdate`
4. Si falla, rollback autom√°tico al commit anterior

### 6.2 Tests de Integraci√≥n (Ejemplo: Orion-LD)

```python
import requests

def test_orion_ngsi_ld_compliance():
    """Verifica que Orion responda seg√∫n ETSI spec"""
    base_url = "https://api.procuredata.com/orion"
    
    # Test: Crear entidad
    entity = {
        "id": "urn:ngsi-ld:Test:001",
        "type": "TestEntity",
        "value": {"type": "Property", "value": 42}
    }
    r = requests.post(f"{base_url}/ngsi-ld/v1/entities", json=entity)
    assert r.status_code == 201
    
    # Test: Recuperar entidad
    r = requests.get(f"{base_url}/ngsi-ld/v1/entities/urn:ngsi-ld:Test:001")
    assert r.status_code == 200
    assert r.json()["value"]["value"] == 42
    
    # Test: Eliminar entidad
    r = requests.delete(f"{base_url}/ngsi-ld/v1/entities/urn:ngsi-ld:Test:001")
    assert r.status_code == 204
```

---

## üîÆ 7. Roadmap de Evoluci√≥n

### Fase Actual (v2.0): Kubernetes Monol√≠tico
- Todos los componentes FIWARE en un cl√∫ster
- Escalado manual (HPA configurado)
- Backups diarios

### v2.5 (Q2 2025): Multi-Region
- Orion-LD replicado en 3 regiones (EU, US, APAC)
- MongoDB global cluster (latencia < 50ms)
- Keyrock federado (SSO entre regiones)

### v3.0 (Q4 2025): Service Mesh (Istio)
- Telemetr√≠a autom√°tica (traces, spans)
- Circuit breakers (fallback si TRUE Connector falla)
- mTLS entre todos los pods (zero-trust)

### v3.5 (2026): Edge Computing
- Orion-LD Lite en dispositivos IoT (ARM64)
- Sincronizaci√≥n bidireccional con cluster central
- 5G network slicing para latencia ultra-baja

---

## üìö 8. Anexos

### 8.1 Glosario

| T√©rmino | Definici√≥n |
|---------|------------|
| **NGSI-LD** | Next Generation Service Interface - Linked Data (est√°ndar ETSI) |
| **ODRL** | Open Digital Rights Language (pol√≠ticas de uso de datos) |
| **IDS** | International Data Spaces (arquitectura de soberan√≠a) |
| **PEP** | Policy Enforcement Point (valida permisos) |
| **DAPS** | Dynamic Attribute Provisioning Service (CA de certificados IDS) |
| **Gemelo Digital** | Representaci√≥n virtual de un objeto f√≠sico (ISO 23247) |

### 8.2 Comparativa: Docker vs. Kubernetes

| Aspecto | Docker Compose | Kubernetes |
|---------|---------------|-----------|
| Despliegue inicial | 5 min | 30 min (incluye cluster setup) |
| Escalado | Manual | Autom√°tico (HPA) |
| Alta disponibilidad | ‚ùå Single host | ‚úÖ Multi-nodo |
| Actualizaciones | Downtime necesario | Rolling updates sin downtime |
| Costo operativo | Bajo (1 servidor) | Medio-Alto (3+ nodos) |
| **Recomendado para** | Dev/Testing | Producci√≥n |

### 8.3 Costos Estimados (AWS EKS)

| Recurso | Cantidad | Costo Mensual (USD) |
|---------|----------|---------------------|
| EKS Control Plane | 1 | $73 |
| Worker Nodes (t3.xlarge) | 3 | $300 |
| MongoDB PVC (gp3 SSD) | 150Gi | $15 |
| MySQL PVC (gp3 SSD) | 60Gi | $6 |
| Application Load Balancer | 1 | $23 |
| Data Transfer (outbound) | 500GB | $45 |
| **TOTAL** | | **~$462/mes** |

**Optimizaci√≥n**: Usar Spot Instances para Orion-LD (-70% costo) ‚Üí **$270/mes**

---

## üÜò Contacto y Soporte

**Equipo de Arquitectura**:
- Email: arquitectura@procuredata.com
- Slack: `#fiware-architecture`
- Docs: https://docs.procuredata.com

**Incidentes cr√≠ticos**:
- On-call: +34 XXX XXX XXX (24/7)
- PagerDuty: https://procuredata.pagerduty.com

---

**Versi√≥n del Documento**: 2.0  
**√öltima Actualizaci√≥n**: Diciembre 2024  
**Pr√≥xima Revisi√≥n**: Marzo 2025  

**Aprobado por**:
- [ ] CTO
- [ ] Arquitecto de Seguridad
- [ ] Lead DevOps Engineer
